{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabridulol/nn-and-dl/blob/main/pa2_multilayer_perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**PA2: Multilayer Perceptron**\n",
        "\n",
        "### **Introduction**\n",
        "In this programming assignment, you will implement the Multilayer Perceptron (MLP) in Python using only the numpy library. This implementation will allow the creation of neural networks with any number of layers. As in the previous project, this model will also be used to solve the problem of classifying cat images. At the end of this activity, you will have a deep neural network with considerably better performance than the logistic regression implemented in the previous project.\n",
        "\n",
        "### **Objective**\n",
        "\n",
        "The main objective of this project is to learn the implementation of the MLP algorithm with a flexible number of layers. Mainly the forward pass and backpropagation of errors. Additionally, you will implement the ReLU activation function and gain experience in comparing neural models of different sizes.\n",
        "\n",
        "### **Instructions**\n",
        "\n",
        "The cells where you need to write code are highlighted with the following comments:\n",
        "\n",
        "```python\n",
        "### YOUR CODE STARTS HERE ### ≈x lines\n",
        "### YOUR CODE ENDS HERE ###\n",
        "```\n",
        "\n",
        "Write your solutions only between these two comments. Note that the starting comment gives you an idea of the number of lines of code expected in the solution.\n",
        "\n",
        "After each cell of this type, there will be a test cell followed by expected results, so you can know if your solution is correct."
      ],
      "metadata": {
        "id": "owgUva0hvM6j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Part 0: Import libraries**\n",
        "\n",
        "As we did in the previous assignment, our first task is to load the numpy library and other auxiliary libraries, which will help us visualize the data and results:\n",
        "\n",
        "- **numpy**: main scientific computing library in Python.\n",
        "- **matplotlib**: main library for plotting graphs in Python.\n",
        "- **h5py**: read datasets in h5 format.\n",
        "- **PIL**: test your model with your own images.\n",
        "\n",
        "Note:\n",
        "As seen in class, we need to initialize the MLP weights with random values close to zero. Therefore, in this project, we initialize the numpy random number generator (np.random.seed(1)) with a fixed seed, so that your results will be the same as the expected results."
      ],
      "metadata": {
        "id": "loAR-kK6zFw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py\n",
        "import scipy\n",
        "from PIL import Image\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "np.random.seed(1)"
      ],
      "metadata": {
        "id": "AVLFnDvM67UD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Part 1: Load and preprocess the dataset**\n",
        "\n",
        "#### **1.1 Download the dataset**\n",
        "\n",
        "The dataset for this project contains:\n",
        "  - Training: $n_{tr}$ images labeled as Cat ($y=1$) or Not-cat ($y=0$)\n",
        "  - Test: $n_{te}$ images labeled as Cat ($y=1$) or Not-cat ($y=0$)\n",
        "  \n",
        "All images are square and colored. Therefore, they are represented by numpy arrays in the form `(num_px, num_px, 3)`, where `num_px` is the width and height of the image, and 3 refers to the color channels (RGB)."
      ],
      "metadata": {
        "id": "G_bnGdtEedNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O 'train_catvnoncat.h5' 'https://raw.githubusercontent.com/JhonataMiranda/CCF482/main/train_catvnoncat.h5'\n",
        "!wget -O 'test_catvnoncat.h5' 'https://raw.githubusercontent.com/JhonataMiranda/CCF482/main/test_catvnoncat.h5'\n",
        "\n",
        "train_dataset = h5py.File('train_catvnoncat.h5', \"r\")\n",
        "train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # training set images\n",
        "train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # training set image labels\n",
        "\n",
        "test_dataset = h5py.File('test_catvnoncat.h5', \"r\")\n",
        "test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # testing set images\n",
        "test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # testing set image labels\n",
        "\n",
        "classes = np.array(test_dataset[\"list_classes\"][:]) # list of classes\n",
        "\n",
        "train_set_y = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
        "test_set_y = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))"
      ],
      "metadata": {
        "id": "0kHPWJZ5eiAZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c56dd477-b349-430f-d144-6b343c03a39f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-02 11:21:13--  https://raw.githubusercontent.com/JhonataMiranda/CCF482/main/train_catvnoncat.h5\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2572022 (2.5M) [application/octet-stream]\n",
            "Saving to: ‘train_catvnoncat.h5’\n",
            "\n",
            "train_catvnoncat.h5 100%[===================>]   2.45M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-04-02 11:21:13 (51.1 MB/s) - ‘train_catvnoncat.h5’ saved [2572022/2572022]\n",
            "\n",
            "--2025-04-02 11:21:14--  https://raw.githubusercontent.com/JhonataMiranda/CCF482/main/test_catvnoncat.h5\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 616958 (602K) [application/octet-stream]\n",
            "Saving to: ‘test_catvnoncat.h5’\n",
            "\n",
            "test_catvnoncat.h5  100%[===================>] 602.50K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-04-02 11:21:14 (18.6 MB/s) - ‘test_catvnoncat.h5’ saved [616958/616958]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.2 Visualization of dataset examples**\n",
        "\n",
        "Each row of `train_set_x_orig` and `test_set_x_orig` is a numpy array representing an image. You can visualize an example by executing the following code. Feel free to change the value of the `index` variable and run again to see other images."
      ],
      "metadata": {
        "id": "RJgKPBK6fDQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting an image from the training set\n",
        "index = 25\n",
        "plt.imshow(train_set_x_orig[index])\n",
        "print (\"y = \" + str(train_set_y[0, index]) + \", it's a '\" + classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") +  \"' picture.\")"
      ],
      "metadata": {
        "id": "uOlFLknufDte",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "1bf4b264-5ab4-4571-df0a-3b6f5d8a5d44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y = 1, it's a 'cat' picture.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.3 Data pre-processing**\n",
        "\n",
        "Just like in the previous project, we also need to process the data in this project. Since the dataset is exactly the same, the following cell performs the data pre-processing exactly as in the previous project."
      ],
      "metadata": {
        "id": "1NCtU4Hrgag5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get number of train examples, test examples and image size\n",
        "m_train = train_set_x_orig.shape[0]\n",
        "m_test = test_set_x_orig.shape[0]\n",
        "num_px = train_set_x_orig.shape[1]\n",
        "\n",
        "# Reshape the training and test examples\n",
        "train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T\n",
        "test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T\n",
        "\n",
        "# Standardize data to have feature values between 0 and 1.\n",
        "train_set_x = train_set_x_flatten/255\n",
        "test_set_x = test_set_x_flatten/255\n",
        "\n",
        "print (\"train_set_x shape: \" + str(train_set_x.shape))\n",
        "print (\"test_set_x shape: \" + str(test_set_x.shape))\n",
        "print (\"train_y shape: \" + str(train_set_y.shape))\n",
        "print (\"test_y shape: \" + str(test_set_y.shape))\n",
        "print (\"Number of training examples: \" + str(m_train))\n",
        "print (\"Number of testing examples: \" + str(m_test))\n",
        "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")"
      ],
      "metadata": {
        "id": "OxOYVzsHgZLQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d608531f-79af-42a6-c0bb-8ca8d155a243"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_set_x shape: (12288, 209)\n",
            "test_set_x shape: (12288, 50)\n",
            "train_y shape: (1, 209)\n",
            "test_y shape: (1, 50)\n",
            "Number of training examples: 209\n",
            "Number of testing examples: 50\n",
            "Each image is of size: (64, 64, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Part 2: Define the model**\n",
        "\n",
        "In this project, we will use the MLP for the classification of cat images. Remember that the MLP organizes neurons in a layered structure. The general formulation of the MLP allows both the number of layers L and the number of neurons $m^{[l]}$ per layer $1 \\leq l \\leq L$ to be configurable according to the complexity of the learning problem. Additionally, the activation functions of the MLP neurons don't necessarily need to be the sigmoid function. We can choose the activation function for each layer of the network. Remember that this general formulation of the MLP has the following hypothesis:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "Z^{[l]} &= W^{[l]} A^{[l-1]} + \\mathbf{b}^{[l]} \\\\\n",
        "\\mathbf{A}^{[l]} &= g^{[l]}(Z^{[l]}) \\\\\n",
        "\\end{align} $$, where:\n",
        "\n",
        "- $W^{[l]}$ is the weight matrix of layer $l$ with dimensions $({m^{[l]}, m^{[l-1]}}$);\n",
        "- $\\mathbf{b}^{[l]}$ is the bias vector of layer $l$ with dimensions $(m^{[l]}, 1)$;\n",
        "- $A^{[L]}$ is the activation matrix of layer $l$ with dimensions $(m^{[l]}, n)$;\n",
        "- $g^{[l]}$ is the activation function of layer $l$.\n",
        "\n",
        "According to this definition, we will have the following dimensions for a network with L layers (Remember that $A^{[0]} = X$ and $A^{[L]} = \\hat{Y}$):\n",
        "\n",
        "<table style=\"width:100%\">\n",
        "    <tr>\n",
        "        <td>  </td>\n",
        "        <td> **  Dimensions of $W^{[l]}$  ** </td>\n",
        "        <td> **  Dimensions of $b^{[l]}$  **  </td>\n",
        "        <td> **  Dimensions of $A^{[l]}$  ** </td>\n",
        "    <tr>\n",
        "    <tr>\n",
        "        <td> ** Layer [1] ** </td>\n",
        "        <td> $(m^{[1]}, 12288)$ </td>\n",
        "        <td> $(m^{[1]}, 1)$ </td>\n",
        "        <td> $(m^{[1]}, 209)$ </td>\n",
        "    <tr>\n",
        "    <tr>\n",
        "        <td> ** Layer [2] ** </td>\n",
        "        <td> $(m^{[2]}, m^{[1]})$  </td>\n",
        "        <td> $(m^{[2]}, 1)$ </td>\n",
        "        <td> $(m^{[2]}, 209)$ </td>\n",
        "    <tr>\n",
        "       <tr>\n",
        "        <td> $\\vdots$ </td>\n",
        "        <td> $\\vdots$  </td>\n",
        "        <td> $\\vdots$  </td>\n",
        "        <td> $\\vdots$  </td>\n",
        "    <tr>\n",
        "   <tr>\n",
        "        <td> ** Layer [L-1] ** </td>\n",
        "        <td> $(m^{[L-1]}, m^{[L-2]})$ </td>\n",
        "        <td> $(m^{[L-1]}, 1)$  </td>\n",
        "        <td> $(m^{[L-1]}, 209)$ </td>\n",
        "    <tr>\n",
        "   <tr>\n",
        "        <td> ** Layer [L] ** </td>\n",
        "        <td> $(m^{[L]}, m^{[L-1]})$ </td>\n",
        "        <td> $(m^{[L]}, 1)$ </td>\n",
        "        <td> $(m^{[L]}, 209)$  </td>\n",
        "    <tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "TuyWSfsFjekr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **2.1 Activation Functions**\n",
        "\n",
        "#### **2.1.1 Sigmoid**\n",
        "\n",
        "The learning problem in this project is the same as in the previous project: binary classification. Therefore, we will use the sigmoid activation function in the output layer and the Binary Cross-Entropy loss function. Since the Sigmoid function was already implemented in the previous project, the following cell already contains the implementation of this function. However, in addition to the activation, the function also returns the value of Z, which we call \"cache,\" as we will use this value during the gradient calculation during backpropagation."
      ],
      "metadata": {
        "id": "WhQ6Acqq1cYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(Z):\n",
        "    \"\"\"\n",
        "    Implements the sigmoid activation in numpy\n",
        "\n",
        "    Arguments:\n",
        "    Z -- numpy array of any shape\n",
        "\n",
        "    Returns:\n",
        "    A -- output of sigmoid(z), same shape as Z\n",
        "    cache -- returns Z as well, useful during backpropagation\n",
        "    \"\"\"\n",
        "\n",
        "    A = 1/(1+np.exp(-Z))\n",
        "    cache = Z\n",
        "\n",
        "    return A, cache"
      ],
      "metadata": {
        "id": "-Nt978ox8Wm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.1.2 Relu**\n",
        "\n",
        "As seen in class, ReLU is currently one of the most popular choices for the activation function of neurons in hidden layers. In this project, we will also use this function in these layers.\n",
        "\n",
        "Implement the ReLU activation function. Note that, just like with the sigmoid, the function returns both the activation and the value of Z. Remember that ReLU is defined by $relu(z) = max(0, z)$. In numpy, the max function is implemented by `np.maximum`."
      ],
      "metadata": {
        "id": "UeC0Aoj08sWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(Z):\n",
        "    \"\"\"\n",
        "    Implement the RELU function.\n",
        "\n",
        "    Arguments:\n",
        "    Z -- Output of the linear layer, of any shape\n",
        "\n",
        "    Returns:\n",
        "    A -- Post-activation parameter, of the same shape as Z\n",
        "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
        "    \"\"\"\n",
        "    ### YOUR CODE STARTS HERE ### ≈1 line\n",
        "\n",
        "    ### YOUR CODE ENDS HERE ###\n",
        "    assert(A.shape == Z.shape)\n",
        "\n",
        "    cache = Z\n",
        "    return A, cache"
      ],
      "metadata": {
        "id": "EIv00d9_8XlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Z = np.array([[-2.0 , 7.0]])\n",
        "A, cache = relu(Z)\n",
        "\n",
        "print(\"Activation: \", A)\n",
        "print(\"Cache: \", cache)"
      ],
      "metadata": {
        "id": "dFFKNd6eNf5I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "72eec873-306b-4157-c227-f6947c0d83d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'A' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-547720d0ab97>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2.0\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Activation: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cache: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-208de47270bc>\u001b[0m in \u001b[0;36mrelu\u001b[0;34m(Z)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m### YOUR CODE ENDS HERE ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'A' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expected result:**\n",
        "<table style=\"width:50%\">\n",
        "  <tr>\n",
        "    <td> **Activation** </td>\n",
        "    <td > [[ 0.  7.]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td> **Cache ** </td>\n",
        "    <td > [[ -2.  7.]] </td>\n",
        "  </tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "sl-VrLQaOCSk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6badpbTcmhQL"
      },
      "source": [
        "### **2.2 - Multilayer Perceptron**\n",
        "Implement the forward propagation of an MLP with L layers. To do this, implement two auxiliary functions: `linear_forward` and `linear_activation_forward`. The first calculates the linear part $Z^{[l]}$ and the second calculates the activation $g^{[l]}(Z^{[l]})$ of layer $l$. This will facilitate the implementation of backpropagation in the next steps."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.1 Linear Combination of Layer $l$**\n",
        "\n",
        "Implement the linear combination $Z^{[l]}$ between the weights and inputs of a layer $l$ of the MLP. Remember that this combination is given by the equation $Z^{[l]} = W^{[l]} A^{[l-1]} + \\mathbf{b}^{[l]}$."
      ],
      "metadata": {
        "id": "Bx-za7R2CD7V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9WDI8eZmhQL"
      },
      "outputs": [],
      "source": [
        "def linear_forward(A, W, b):\n",
        "    \"\"\"\n",
        "    Implement the linear part of a layer's forward propagation.\n",
        "\n",
        "    Arguments:\n",
        "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
        "\n",
        "    Returns:\n",
        "    Z -- the input of the activation function, also called pre-activation parameter\n",
        "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
        "    \"\"\"\n",
        "\n",
        "    ### YOUR CODE STARTS HERE ### ≈1 line\n",
        "\n",
        "    ### YOUR CODE ENDS HERE ###\n",
        "\n",
        "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
        "    cache = (A, W, b)\n",
        "\n",
        "    return Z, cache"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "\n",
        "A = np.random.randn(3,2); W = np.random.randn(1,3); b = np.random.randn(1,1)\n",
        "Z, linear_cache = linear_forward(A, W, b)\n",
        "print(\"Z = \" + str(Z))"
      ],
      "metadata": {
        "id": "wmZvqYwyOYxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expected result:**\n",
        "\n",
        "<table style=\"width:35%\">\n",
        "  \n",
        "  <tr>\n",
        "    <td> **Z** </td>\n",
        "    <td> [[ 3.26295337 -1.23429987]] </td>\n",
        "  </tr>\n",
        "  \n",
        "</table>"
      ],
      "metadata": {
        "id": "bo5zr_QgOZam"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.2 Activation of Layer $l$**\n",
        "\n",
        "Implement the activation $g^{[l]}(Z^{[l]})$ of layer $l$ of the MLP. Although the ReLU activation function in hidden layers makes the learning process faster than the sigmoid, we will also support the use of sigmoid in these layers, so we can conduct experiments with different activations. Remember that this combination is given by the equation $g^{[l]}(Z^{[l]})$,\n",
        "where $Z^{[l]}$ is calculated by the `linear_forward` function implemented in the previous step and $g^{[l]}$ can be either `sigmoid` or `relu`, depending\n",
        "on the `activation` parameter (string)."
      ],
      "metadata": {
        "id": "dmNKH6pPC774"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MR5jUje2mhQM"
      },
      "outputs": [],
      "source": [
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "    \"\"\"\n",
        "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
        "\n",
        "    Arguments:\n",
        "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
        "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
        "\n",
        "    Returns:\n",
        "    A -- the output of the activation function, also called the post-activation value\n",
        "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
        "             stored for computing the backward pass efficiently\n",
        "    \"\"\"\n",
        "\n",
        "    if activation == \"sigmoid\":\n",
        "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
        "        ### YOUR CODE STARTS HERE ### ~2 lines\n",
        "\n",
        "        ### YOUR CODE ENDS HERE ###\n",
        "\n",
        "\n",
        "    elif activation == \"relu\":\n",
        "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
        "        ### YOUR CODE STARTS HERE ### ~2 lines\n",
        "\n",
        "        ### YOUR CODE ENDS HERE ###\n",
        "\n",
        "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
        "    cache = (linear_cache, activation_cache)\n",
        "\n",
        "    return A, cache"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(2)\n",
        "A_prev = np.random.randn(3,2); W = np.random.randn(1,3); b = np.random.randn(1,1)\n",
        "\n",
        "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n",
        "print(\"Activation (sigmoid) = \" + str(A))\n",
        "\n",
        "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n",
        "print(\"Activation (relu) = \" + str(A))"
      ],
      "metadata": {
        "id": "aoOs7R4LO--K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expected result:**       \n",
        "<table style=\"width:35%\">\n",
        "  <tr>\n",
        "    <td> **Activation (sigmoid) ** </td>\n",
        "    <td > [[ 0.96890023  0.11013289]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td> **Activation (relu) ** </td>\n",
        "    <td > [[ 3.43896131  0.        ]]</td>\n",
        "  </tr>\n",
        "</table>\n"
      ],
      "metadata": {
        "id": "CexcNOWJPUv-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJrwrlewmhQM"
      },
      "source": [
        "### **2.3 Forward Propagation (Forward Pass)**\n",
        "\n",
        "The `linear_activation_forward` function calculates the activation of a single layer $l$. Use this function to implement the `forward_pass` function, which performs propagation of the inputs $X$ through all layers of the network. Schematically, the architecture of the MLP with L layers can be seen as follows:\n",
        "\n",
        "$X$ $→$ (Linear | ReLU) $\\times$ [L-1] $→$ (Linear | Sigmoid) $→$ $\\hat{Y}$\n",
        "\n",
        "First, calculate the activation $A^{[l]}$ for the [L-1] hidden layers (Linear $→$ ReLU) and then calculate the activation $A^{[L]} = \\hat{Y}$ of the output layer. To calculate the activation of a layer $l$ with the `linear_activation_forward` function, you need the weights $W^{[l]}$ and $\\mathbf{b}^{[l]}$ of that layer. The `parameters` variable of the `forward_pass` function stores the weights of all layers in a dictionary. To access the weights of a given layer $l$, simply create a string key with the parameter name and the layer number. For example, `parameters['W1']` and `parameters['b1']` contain the parameters $W^{[1]}$ and $\\mathbf{b}^{[1]}$ of layer 1, respectively.\n",
        "\n",
        "Remember that, in addition to the activation $A^{[l]}$, the `linear_activation_forward` function returns the value of $Z^{[l]}$ in a cache. As we will need these values during backprop, store (`cache.append()`) the outputs of each layer in the `caches` list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0nWcsPcmhQM"
      },
      "outputs": [],
      "source": [
        "def forward_pass(X, parameters):\n",
        "    \"\"\"\n",
        "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
        "\n",
        "    Arguments:\n",
        "    X -- data, numpy array of shape (input size, number of examples)\n",
        "    parameters -- output of initialize_parameters_deep()\n",
        "\n",
        "    Returns:\n",
        "    Y_hat -- last post-activation value\n",
        "    caches -- list of caches containing:\n",
        "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
        "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
        "    \"\"\"\n",
        "\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(parameters) // 2 # number of layers in the neural network\n",
        "\n",
        "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
        "    for l in range(1, L):\n",
        "        A_prev = A\n",
        "        ### YOUR CODE STARTS HERE ### ~2 lines\n",
        "\n",
        "        ### YOUR CODE ENDS HERE ###\n",
        "\n",
        "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
        "    ### YOUR CODE STARTS HERE ### ~2 lines\n",
        "\n",
        "    ### YOUR CODE ENDS HERE ###\n",
        "\n",
        "    assert(Y_hat.shape == (1,X.shape[1]))\n",
        "\n",
        "    return Y_hat, caches"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(6)\n",
        "X = np.random.randn(5,4)\n",
        "W1 = np.random.randn(4,5); b1 = np.random.randn(4,1)\n",
        "W2 = np.random.randn(3,4); b2 = np.random.randn(3,1)\n",
        "W3 = np.random.randn(1,3); b3 = np.random.randn(1,1)\n",
        "parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2, \"W3\": W3, \"b3\": b3}\n",
        "\n",
        "Y_hat, caches = forward_pass(X, parameters)\n",
        "print(\"Y_hat = \" + str(Y_hat))\n",
        "print(\"Cache length = \" + str(len(caches)))"
      ],
      "metadata": {
        "id": "knW56iT9j7FD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expected result:**       \n",
        "\n",
        "<table style=\"width:50%\">\n",
        "  <tr>\n",
        "    <td> **Y_hat** </td>\n",
        "    <td > [[ 0.03921668  0.70498921  0.19734387  0.04728177]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td> **Cache length ** </td>\n",
        "    <td > 3 </td>\n",
        "  </tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "cGiQ_aYGkZ8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Part 3: Initialize weights**\n",
        "\n",
        "Unlike logistic regression, in MLP we cannot initialize the weights $W^{[l]}$ of a layer $l$ with zeros, as this would make the neurons in the same layer identical. In MLP, weights are initialized with random values close to zero.\n",
        "\n",
        "In class, we saw a simple solution that was to generate a random matrix with values between 0 and 1 and multiply it by a small constant (e.g., 0.01). However, a better solution is to generate a random matrix with values between 0 and 1 and divide it by the square root of the number of inputs $m[^{[l-1]}]$ of layer $l$. This helps ensure that the output of a neuron is not dominated by a small number of inputs, which could lead to overfitting. When a neuron receives a large number of inputs, the output can be highly variable. This occurs because some inputs may have a much greater impact on the results than others. For example, if a neuron receives 100 inputs and one of them is 10 times larger than the other 99, then that input will dominate the output. To avoid this problem, the weights of the inputs are divided by the square root of the number of inputs.\n",
        "\n",
        "Implement the following function to initialize the MLP weights using this technique. To generate a random matrix with values between 0 and 1, use the function `np.random.randn(m_l, m_{l-1})`. To calculate the square root of the number of inputs of a layer, use the function `np.sqrt(m_{l-1})`. The weights $b^{[l]}$ can be initialized with zero. For this, use the function `np.zeros((m_l, 1))`. Note that this function receives as a parameter a list `layer_dims` where an element with index $l$ contains the number of neurons in layer $l$."
      ],
      "metadata": {
        "id": "FmqVtM8kGUjs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pe_I6GFUmhQL"
      },
      "outputs": [],
      "source": [
        "def initialize_parameters(layer_dims):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
        "\n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
        "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
        "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
        "    \"\"\"\n",
        "\n",
        "    np.random.seed(1)\n",
        "    parameters = {}\n",
        "    L = len(layer_dims) # number of layers in the network\n",
        "\n",
        "    for l in range(1, L):\n",
        "        ### YOUR CODE STARTS HERE ### ~2 lines\n",
        "\n",
        "        ### YOUR CODE ENDS HERE ###\n",
        "\n",
        "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
        "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
        "\n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_VzRJIsmhQL"
      },
      "outputs": [],
      "source": [
        "parameters = initialize_parameters([5,4,3])\n",
        "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
        "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
        "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
        "print(\"b2 = \" + str(parameters[\"b2\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rI2TWJcmhQL"
      },
      "source": [
        "**Expected result:**       \n",
        "       \n",
        "<table style=\"width:80%\">\n",
        "  <tr>\n",
        "    <td> **W1** </td>\n",
        "    <td>[[ 0.72642933 -0.27358579 -0.23620559 -0.47984616  0.38702206]\n",
        " [-1.0292794   0.78030354 -0.34042208  0.14267862 -0.11152182]\n",
        " [ 0.65387455 -0.92132293 -0.14418936 -0.17175433  0.50703711]\n",
        " [-0.49188633 -0.07711224 -0.39259022  0.01887856  0.26064289]]</td>\n",
        "  </tr>\n",
        "  \n",
        "  <tr>\n",
        "    <td>**b1** </td>\n",
        "    <td>[[ 0.]\n",
        " [ 0.]\n",
        " [ 0.]\n",
        " [ 0.]]</td>\n",
        "  </tr>\n",
        "  \n",
        "  <tr>\n",
        "    <td>**W2** </td>\n",
        "    <td>[[-0.55030959  0.57236185  0.45079536  0.25124717]\n",
        " [ 0.45042797 -0.34186393 -0.06144511 -0.46788472]\n",
        " [-0.13394404  0.26517773 -0.34583038 -0.19837676]]</td>\n",
        "  </tr>\n",
        "  \n",
        "  <tr>\n",
        "    <td>**b2** </td>\n",
        "    <td>[[ 0.]\n",
        " [ 0.]\n",
        " [ 0.]]</td>\n",
        "  </tr>\n",
        "  \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FD2zeC3bmhQN"
      },
      "source": [
        "### **Part 4: Define loss function**\n",
        "\n",
        "As we have a binary classification problem, we will use Binary Cross-Entropy as the loss function. In MLP, the optimization problem with this function is no longer convex, but gradient descent still works very well to optimize the network weights. Remember that Binary Cross-Entropy is defined as follows:\n",
        "\n",
        "$$\n",
        "L(h) = -\\frac{1}{m}\\sum_{i=1}^{m}(y_i\\;log\\ \\hat{y_i} + (1 - y_i)\\;log\\;(1 - \\hat{y_i}))\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDUmAXH_mhQN"
      },
      "outputs": [],
      "source": [
        "def binary_cross_entropy(Y_hat, Y):\n",
        "    \"\"\"\n",
        "    Implement the cost function defined by equation (7).\n",
        "\n",
        "    Arguments:\n",
        "    Y_hat -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
        "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
        "\n",
        "    Returns:\n",
        "    cost -- cross-entropy cost\n",
        "    \"\"\"\n",
        "\n",
        "    m = Y.shape[1]\n",
        "\n",
        "    # Compute loss from aL and y.\n",
        "    loss = (1./m) * (-np.dot(Y,np.log(Y_hat).T) - np.dot(1-Y, np.log(1-Y_hat).T))\n",
        "\n",
        "    loss = np.squeeze(loss) # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
        "    assert(loss.shape == ())\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JXqLogjmhQN"
      },
      "source": [
        "### **Part 5: Backpropagation**\n",
        "\n",
        "The weights of an MLP are updated with the Backpropagation algorithm.\n",
        "This algorithm efficiently calculates the partial derivatives of the error function with respect to the network weights by applying the chain rule from back to front, starting at the output layer and going back to the input layer. As we saw in class, each node in our computational graph needs to know how to compute it's local gradients, which are the partial derivatives of it's output with respect to it's inputs. In this programming assigment, we have four nodes in our computational graph: `linear(A, w, b)`, `sigmoid(z)`, `relu(z)`, and `binary_cross_entropy(y_hat, y)`. Thus, we need to define the local gradients of these four nodes. Since we already computed the partial derivatives of the binary cross entropy loss for these four nodes in class, we will compute their global gradients directly.\n",
        "\n",
        "First, you will implement the `linear_backward` function to calculate the partial derivatives of the loss function $L$ with respect to the weights ($W^{[l]}$, $b^{[l]}$) and the activation of the previous layer $A^{[l-1]}$. Second, the `sigmoid_backward` and `relu_backward` functions to calculate the derivatives of the activation functions with respect to the inputs Z. Then, you will implement the `linear_activation_backward` function, which combines these functions to calculate the backpropagation for a single layer $l$.\n",
        "Finally, you will implement the `backward_pass` function, which uses\n",
        "the `linear_activation_backward` function to calculate the backpropagation in all layers of the MLP."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5.1 Partial Derivatives of the Linear Combination of Layer $l$**\n",
        "\n",
        "Partial derivatives of the loss function $L$ with respect to the weights ($W^{[l]}$, $b^{[l]}$) of a layer $l$ of the MLP:\n",
        "\n",
        "$$ dW^{[l]} = \\frac{\\partial L}{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} $$\n",
        "$$ db^{[l]} = \\frac{\\partial L}{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)} $$\n",
        "$$ dA^{[l-1]} = \\frac{\\partial L}{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} $$"
      ],
      "metadata": {
        "id": "HSorv_L6HRpG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LgcfFKvmhQN"
      },
      "outputs": [],
      "source": [
        "def linear_backward(dZ, cache):\n",
        "    \"\"\"\n",
        "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
        "\n",
        "    Arguments:\n",
        "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
        "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
        "\n",
        "    Returns:\n",
        "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "    \"\"\"\n",
        "    A_prev, W, b = cache\n",
        "    m = A_prev.shape[1]\n",
        "\n",
        "    ### YOUR CODE STARTS HERE ### ~3 lines\n",
        "\n",
        "    ### YOUR CODE ENDS HERE ###\n",
        "\n",
        "    assert (dA_prev.shape == A_prev.shape)\n",
        "    assert (dW.shape == W.shape)\n",
        "    assert (db.shape == b.shape)\n",
        "\n",
        "    return dA_prev, dW, db"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up some test inputs\n",
        "np.random.seed(1)\n",
        "\n",
        "dZ = np.random.randn(3,4)\n",
        "A = np.random.randn(5,4)\n",
        "W = np.random.randn(3,5)\n",
        "b = np.random.randn(3,1)\n",
        "linear_cache = (A, W, b)\n",
        "\n",
        "dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "print (\"dA_prev = \"+ str(dA_prev))\n",
        "print (\"dW = \" + str(dW))\n",
        "print (\"db = \" + str(db))"
      ],
      "metadata": {
        "id": "svgo3Y29nqwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expected result:**       \n",
        "       \n",
        "<table style=\"width:80%\">\n",
        "  <tr>\n",
        "    <td> **dA_prev** </td>\n",
        "    <td>[[-1.15171336  0.06718465 -0.3204696   2.09812712]\n",
        " [ 0.60345879 -3.72508701  5.81700741 -3.84326836]\n",
        " [-0.4319552  -1.30987417  1.72354705  0.05070578]\n",
        " [-0.38981415  0.60811244 -1.25938424  1.47191593]\n",
        " [-2.52214926  2.67882552 -0.67947465  1.48119548]]</td>\n",
        "  </tr>\n",
        "  \n",
        "  <tr>\n",
        "    <td>**dW** </td>\n",
        "    <td>[[ 0.07313866 -0.0976715  -0.87585828  0.73763362  0.00785716]\n",
        " [ 0.85508818  0.37530413 -0.59912655  0.71278189 -0.58931808]\n",
        " [ 0.97913304 -0.24376494 -0.08839671  0.55151192 -0.10290907]]</td>\n",
        "  </tr>\n",
        "  \n",
        "  <tr>\n",
        "    <td>**db** </td>\n",
        "    <td>[[-0.14713786]\n",
        " [-0.11313155]\n",
        " [-0.13209101]]</td>\n",
        "  </tr>\n",
        "\n",
        "  \n",
        "</table>"
      ],
      "metadata": {
        "id": "UiYFRAaXSgrd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5.2 Derivatives of activation functions**\n",
        "\n",
        "Derivatives of the sigmoid function with respect to input Z:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial g}{\\partial Z} = \\frac{\\partial L}{\\partial a} \\cdot (g(z) \\cdot (1 - g(z))\n",
        "$$"
      ],
      "metadata": {
        "id": "lewT9kBwIzXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid_backward(dA, cache):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for a single SIGMOID unit.\n",
        "\n",
        "    Arguments:\n",
        "    dA -- post-activation gradient, of any shape\n",
        "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
        "\n",
        "    Returns:\n",
        "    dZ -- Gradient of the cost with respect to Z\n",
        "    \"\"\"\n",
        "\n",
        "    Z = cache\n",
        "\n",
        "    g_z = 1/(1 + np.exp(-Z))\n",
        "    ### YOUR CODE STARTS HERE ### ~1 line\n",
        "\n",
        "    ### YOUR CODE ENDS HERE ###\n",
        "\n",
        "    assert (dZ.shape == Z.shape)\n",
        "\n",
        "    return dZ"
      ],
      "metadata": {
        "id": "jIg1pECRD6g1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "\n",
        "dA = np.random.randn(3,4)\n",
        "cache = np.random.randn(3,4)\n",
        "\n",
        "dZ = sigmoid_backward(dA, cache)\n",
        "print('dZ: ', dZ)"
      ],
      "metadata": {
        "id": "Wg4D3A51TiGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expected result:**       \n",
        "       \n",
        "<table style=\"width:80%\">\n",
        "  <tr>\n",
        "    <td> **dZ** </td>\n",
        "    <td>[[ 0.39571307 -0.14743535 -0.09728415 -0.20105294]\n",
        " [ 0.21475173 -0.47735759  0.43600867 -0.17501431]\n",
        " [ 0.05975979 -0.04567319  0.30026189 -0.48384433]]</td>\n",
        "  </tr>\n",
        "  \n",
        "</table>"
      ],
      "metadata": {
        "id": "BIE57ao7ULpd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Derivative of the ReLU function with respect to input Z:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial g}{\\partial Z} = \\begin{cases}\n",
        "0\\, \\text{if}\\ z < 0 \\\\\n",
        "1\\, \\text{if}\\ z > 0 \\\\\n",
        "\\nexists, \\text{if}\\ z = 0 \\\\\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Note that, mathematically, the derivative of ReLU is undefined for $Z = 0$. In practice, we will assume that it is equal to zero."
      ],
      "metadata": {
        "id": "MoqsqyaHRB6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu_backward(dA, cache):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for a single RELU unit.\n",
        "\n",
        "    Arguments:\n",
        "    dA -- post-activation gradient, of any shape\n",
        "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
        "\n",
        "    Returns:\n",
        "    dZ -- Gradient of the cost with respect to Z\n",
        "    \"\"\"\n",
        "\n",
        "    Z = cache\n",
        "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
        "\n",
        "    # When z <= 0, you should set dz to 0 as well.\n",
        "    ### YOUR CODE STARTS HERE ### ~1 line\n",
        "\n",
        "    ### YOUR CODE ENDS HERE ###\n",
        "\n",
        "    assert (dZ.shape == Z.shape)\n",
        "\n",
        "    return dZ"
      ],
      "metadata": {
        "id": "A6oxrtdfD4hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "\n",
        "dA = np.random.randn(3,4)\n",
        "cache = np.random.randn(3,4)\n",
        "\n",
        "dZ = relu_backward(dA, cache)\n",
        "print('dZ: ', dZ)"
      ],
      "metadata": {
        "id": "-7GmrypkUXrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expected result:**       \n",
        "       \n",
        "<table style=\"width:80%\">\n",
        "  <tr>\n",
        "    <td> **dZ** </td>\n",
        "    <td>[[ 0.          0.         -0.52817175  0.        ]\n",
        " [ 0.          0.          1.74481176 -0.7612069 ]\n",
        " [ 0.         -0.24937038  1.46210794 -2.06014071]]</td>\n",
        "  </tr>\n",
        "  \n",
        "</table>"
      ],
      "metadata": {
        "id": "towh0JcrUfaX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5.3 Partial Derivatives of the Activation of Layer $l$**"
      ],
      "metadata": {
        "id": "GAMylWhNSKG_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "057W5sSCmhQO"
      },
      "outputs": [],
      "source": [
        "def linear_activation_backward(dA, cache, activation):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
        "\n",
        "    Arguments:\n",
        "    dA -- post-activation gradient for current layer l\n",
        "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
        "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
        "\n",
        "    Returns:\n",
        "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "    \"\"\"\n",
        "    linear_cache, activation_cache = cache\n",
        "\n",
        "    if activation == \"relu\":\n",
        "        ### YOUR CODE STARTS HERE ### ~2 lines\n",
        "\n",
        "        ### YOUR CODE ENDS HERE ###\n",
        "\n",
        "    elif activation == \"sigmoid\":\n",
        "        ### YOUR CODE STARTS HERE ### ~2 lines\n",
        "\n",
        "        ### YOUR CODE ENDS HERE ###\n",
        "\n",
        "    return dA_prev, dW, db"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPJuJGkHmhQO"
      },
      "source": [
        "### **5.4: Backpropagation**\n",
        "\n",
        "The `linear_activation_backward` function calculates the backpropagation for a single layer $l$. Use this function to implement the `backward_pass` function, which performs the backpropagation of errors $L(\\hat{Y}, Y)$ through all layers of the network, from back to front. Schematically, the backpropagation process can be seen as follows:\n",
        "\n",
        "(Linear | ReLU) $\\times$ [L-1] $←$ (Linear | Sigmoid) $←$ $L(\\hat{Y}, Y)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2Q4P78-mhQP"
      },
      "outputs": [],
      "source": [
        "def backward_pass(Y_hat, Y, caches):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
        "\n",
        "    Arguments:\n",
        "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
        "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
        "    caches -- list of caches containing:\n",
        "                every cache of linear_activation_forward() with \"relu\" (there are (L-1) or them, indexes from 0 to L-2)\n",
        "                the cache of linear_activation_forward() with \"sigmoid\" (there is one, index L-1)\n",
        "\n",
        "    Returns:\n",
        "    grads -- A dictionary with the gradients\n",
        "             grads[\"dA\" + str(l)] = ...\n",
        "             grads[\"dW\" + str(l)] = ...\n",
        "             grads[\"db\" + str(l)] = ...\n",
        "    \"\"\"\n",
        "    grads = {}\n",
        "    L = len(caches) # the number of layers\n",
        "    Y = Y.reshape(Y_hat.shape) # after this line, Y is the same shape as AL\n",
        "\n",
        "    # Compute d_Yhat to initialize the backpropagation\n",
        "    ### YOUR CODE STARTS HERE ### ~1 line\n",
        "\n",
        "    ### YOUR CODE ENDS HERE ###\n",
        "\n",
        "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"Y_hat, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
        "    ### YOUR CODE STARTS HERE ### ~2 lines\n",
        "\n",
        "    ### YOUR CODE ENDS HERE ###\n",
        "\n",
        "    for l in reversed(range(L-1)):\n",
        "        # lth layer: (RELU -> LINEAR) gradients.\n",
        "        ### YOUR CODE STARTS HERE ### ~5 lines\n",
        "\n",
        "        ### YOUR CODE ENDS HERE ###\n",
        "\n",
        "    return grads"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(3)\n",
        "Y_hat = np.random.randn(1, 2); Y = np.array([[1, 0]])\n",
        "\n",
        "A1 = np.random.randn(4,2); W1 = np.random.randn(3,4); b1 = np.random.randn(3,1); Z1 = np.random.randn(3,2)\n",
        "linear_cache_activation_1 = ((A1, W1, b1), Z1)\n",
        "\n",
        "A2 = np.random.randn(3,2); W2 = np.random.randn(1,3); b2 = np.random.randn(1,1); Z2 = np.random.randn(1,2)\n",
        "linear_cache_activation_2 = ((A2, W2, b2), Z2)\n",
        "\n",
        "caches = (linear_cache_activation_1, linear_cache_activation_2)\n",
        "\n",
        "grads = backward_pass(Y_hat, Y, caches)\n",
        "\n",
        "print (\"dW1 = \"+ str(grads[\"dW1\"]))\n",
        "print (\"db1 = \"+ str(grads[\"db1\"]))\n",
        "print (\"dA1 = \"+ str(grads[\"dA1\"]))"
      ],
      "metadata": {
        "id": "0zN3Txs6cwFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expected result:**       \n",
        "\n",
        "<table style=\"width:60%\">\n",
        "  <tr>\n",
        "    <td > dW1 </td>\n",
        "           <td > [[0.41010002 0.07807203 0.13798444 0.10502167]\n",
        " [0.         0.         0.         0.        ]\n",
        " [0.05283652 0.01005865 0.01777766 0.0135308 ]] </td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td > db1 </td>\n",
        "           <td > [[-0.22007063]\n",
        " [ 0.        ]\n",
        " [-0.02835349]] </td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "  <td > dA1 </td>\n",
        "           <td > [[ 0.          0.52257901]\n",
        " [ 0.         -0.3269206 ]\n",
        " [ 0.         -0.32070404]\n",
        " [ 0.         -0.74079187]] </td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n"
      ],
      "metadata": {
        "id": "9Vb_GZjedlFf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Part 6: Optimize weights with gradient descent**\n",
        "\n",
        "Before implementing the main gradient descent loop, implement an auxiliary function `update_parameters` to update the parameters of each layer $l$ of the MLP with their respective gradients. Remember that the gradient descent update rule is as follows:\n",
        "\n",
        "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{16}$$\n",
        "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{17}$$\n",
        "\n",
        "where $\\alpha$ is the learning rate. Remember that all MLP parameters are stored in the `parameters` dictionary and the gradients in the `grads` dictionary."
      ],
      "metadata": {
        "id": "l3oj04bjgVKo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-N2Gu3GmhQP"
      },
      "outputs": [],
      "source": [
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    \"\"\"\n",
        "    Update parameters using gradient descent\n",
        "\n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters\n",
        "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
        "\n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters\n",
        "                  parameters[\"W\" + str(l)] = ...\n",
        "                  parameters[\"b\" + str(l)] = ...\n",
        "    \"\"\"\n",
        "\n",
        "    L = len(parameters) // 2 # number of layers in the neural network\n",
        "\n",
        "    # Update rule for each parameter. Use a for loop.\n",
        "    ### YOUR CODE STARTS HERE ### ~3 lines\n",
        "\n",
        "    ### YOUR CODE ENDS HERE ###\n",
        "\n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(2)\n",
        "W1 = np.random.randn(3,4); b1 = np.random.randn(3,1)\n",
        "W2 = np.random.randn(1,3); b2 = np.random.randn(1,1)\n",
        "parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
        "\n",
        "np.random.seed(3)\n",
        "dW1 = np.random.randn(3,4); db1 = np.random.randn(3,1)\n",
        "dW2 = np.random.randn(1,3); db2 = np.random.randn(1,1)\n",
        "grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
        "\n",
        "parameters = update_parameters(parameters, grads, 0.1)\n",
        "\n",
        "print (\"W1 = \"+ str(parameters[\"W1\"]))\n",
        "print (\"b1 = \"+ str(parameters[\"b1\"]))\n",
        "print (\"W2 = \"+ str(parameters[\"W2\"]))\n",
        "print (\"b2 = \"+ str(parameters[\"b2\"]))"
      ],
      "metadata": {
        "id": "4drZvdHBhQ-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expected result:**       \n",
        "\n",
        "<table style=\"width:100%\">\n",
        "    <tr>\n",
        "    <td > W1 </td>\n",
        "           <td > [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
        " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
        " [-1.0535704  -0.86128581  0.68284052  2.20374577]] </td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td > b1 </td>\n",
        "           <td > [[-0.04659241]\n",
        " [-1.28888275]\n",
        " [ 0.53405496]] </td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td > W2 </td>\n",
        "           <td > [[-0.55569196  0.0354055   1.32964895]]</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td > b2 </td>\n",
        "           <td > [[-0.84610769]] </td>\n",
        "  </tr>\n",
        "</table>\n"
      ],
      "metadata": {
        "id": "Y8ge6K4ShkqS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the main gradient descent loop using the following functions implemented throughout this project: `initialize_parameters`, `forward_pass`, `binary_cross_entropy`, `backward_pass`, and `update_parameters`."
      ],
      "metadata": {
        "id": "MxX-APMXhIMf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3-9Kz4GmrH0"
      },
      "outputs": [],
      "source": [
        "def optimize(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
        "    \"\"\"\n",
        "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
        "\n",
        "    Arguments:\n",
        "    X -- data, numpy array of shape (num_px * num_px * 3, number of examples)\n",
        "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
        "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
        "    learning_rate -- learning rate of the gradient descent update rule\n",
        "    num_iterations -- number of iterations of the optimization loop\n",
        "    print_cost -- if True, it prints the cost every 100 steps\n",
        "\n",
        "    Returns:\n",
        "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
        "    \"\"\"\n",
        "\n",
        "    np.random.seed(1)\n",
        "    losses = []                         # keep track of cost\n",
        "\n",
        "    # Parameters initialization.\n",
        "    ### YOUR CODE STARTS HERE ### ~1 line\n",
        "\n",
        "    ### YOUR CODE ENDS HERE ###\n",
        "\n",
        "    # Loop (gradient descent)\n",
        "    for i in range(0, num_iterations):\n",
        "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
        "        ### YOUR CODE STARTS HERE ### ~1 line\n",
        "\n",
        "        ### YOUR CODE ENDS HERE ###\n",
        "\n",
        "        # Compute cost.\n",
        "        ### YOUR CODE STARTS HERE ### ~1 line\n",
        "\n",
        "        ### YOUR CODE ENDS HERE ###\n",
        "\n",
        "        # Backward propagation.\n",
        "        ### YOUR CODE STARTS HERE ### ~1 line\n",
        "\n",
        "        ### YOUR CODE ENDS HERE ###\n",
        "\n",
        "        # Update parameters.\n",
        "        ### YOUR CODE STARTS HERE ### ~1 line\n",
        "\n",
        "        ### YOUR CODE ENDS HERE ###\n",
        "\n",
        "        # Print the cost every 100 training example\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Loss after iteration %i: %f\" %(i, loss))\n",
        "        if print_cost and i % 100 == 0:\n",
        "            losses.append(loss)\n",
        "\n",
        "    return parameters, losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVRewwpNmrH0"
      },
      "source": [
        "Next, you will use the `optimize` function to train a 2-layer MLP and plot its learning curve."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "Eb6Nd_A4mrH1"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.0075\n",
        "layers_dims = [12288, 7, 1]\n",
        "\n",
        "parameters_2layers, losses_2layers = optimize(train_x, train_y, layers_dims, learning_rate, num_iterations = 2500, print_cost = True)\n",
        "\n",
        "# plot the cost\n",
        "plt.plot(np.squeeze(losses_2layers))\n",
        "plt.ylabel('cost')\n",
        "plt.xlabel('iterations (per hundreds)')\n",
        "plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expected result:**       \n",
        "<table>\n",
        "    <tr>\n",
        "        <td> **Custo após iteração 0**</td>\n",
        "        <td> 0.695046 </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td> **Custo após iteração 100**</td>\n",
        "        <td> 0.589260 </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td> **...**</td>\n",
        "        <td> ... </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td> **Custo após iteração 2400**</td>\n",
        "        <td> 0.026615 </td>\n",
        "    </tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "3WRuaOYLkBla"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now use the same `optimize` function to train a 4-layer MLP and plot its learning curve."
      ],
      "metadata": {
        "id": "_XLOOHvgjZ8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  modelo com 4-camadas\n",
        "learning_rate = 0.0075\n",
        "layers_dims = [12288, 20, 7, 5, 1]\n",
        "\n",
        "parameters_4layers, losses_4layers = optimize(train_x, train_y, layers_dims, learning_rate, num_iterations = 2500, print_cost = True)\n",
        "\n",
        "# plot the cost\n",
        "plt.plot(np.squeeze(losses_4layers))\n",
        "plt.ylabel('cost')\n",
        "plt.xlabel('iterations (per hundreds)')\n",
        "plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5pFAiienet4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9XNJRQamrH1"
      },
      "source": [
        "**Expected result:**       \n",
        "<table>\n",
        "    <tr>\n",
        "        <td> **Cost after iteration 0**</td>\n",
        "        <td> 0.771749 </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td> **Cost after iteration 100**</td>\n",
        "        <td> 0.672053 </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td> **...**</td>\n",
        "        <td> ... </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td> **Cost after iteration 2400**</td>\n",
        "        <td> 0.092878 </td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Part 7: Evaluate trained models**\n",
        "\n",
        "Finally, apply the threshold $\\hat{y} =\n",
        "\\begin{cases}\n",
        "1,\\ \\text{if}\\ h(x) \\geq 0.5\\\\\n",
        "0,\\  \\text{if}\\ h(x) < 0.5\\\\\n",
        "\\end{cases}$ to classify all examples in a matrix X of examples."
      ],
      "metadata": {
        "id": "dWZSTNfAYxXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X, y, parameters):\n",
        "    \"\"\"\n",
        "    This function is used to predict the results of a  L-layer neural network.\n",
        "\n",
        "    Arguments:\n",
        "    X -- data set of examples you would like to label\n",
        "    parameters -- parameters of the trained model\n",
        "\n",
        "    Returns:\n",
        "    p -- predictions for the given dataset X\n",
        "    accuracy -- percentage of examples correcly labelled\n",
        "    \"\"\"\n",
        "\n",
        "    m = X.shape[1]\n",
        "    n = len(parameters) // 2 # number of layers in the neural network\n",
        "    p = np.zeros((1,m))\n",
        "\n",
        "    # Forward propagation\n",
        "    ### YOUR CODE STARTS HERE ### ~1 line\n",
        "\n",
        "    ### YOUR CODE ENDS HERE ###\n",
        "\n",
        "    # Convert probs to 0/1 predictions\n",
        "    ### YOUR CODE STARTS HERE ### ~3-5 lines\n",
        "\n",
        "    ### YOUR CODE ENDS HERE ###\n",
        "\n",
        "    # Compute accuracy\n",
        "    ### YOUR CODE STARTS HERE ### ~1 line\n",
        "\n",
        "    ### YOUR CODE ENDS HERE ###\n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "ZhghYM1QsIZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "UQ6sIaKAmrH1"
      },
      "outputs": [],
      "source": [
        "accuracy_train_2layers = predict(train_x, train_y, parameters_2layers)\n",
        "accuracy_test_2layers = predict(test_x, test_y, parameters_2layers)\n",
        "\n",
        "accuracy_train_4layers = predict(train_x, train_y, parameters_4layers)\n",
        "accuracy_test_4layers = predict(test_x, test_y, parameters_4layers)\n",
        "\n",
        "print(\"2 Layer Train Accuracy:\", accuracy_train_2layers)\n",
        "print(\"2 Layer Test Accuracy:\", accuracy_test_2layers)\n",
        "print(\"4 Layer Train Accuracy:\", accuracy_train_4layers)\n",
        "print(\"4 Layer Test Accuracy:\", accuracy_test_4layers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCUTZ-TvmrH1"
      },
      "source": [
        "**Expected result:**       \n",
        "<table>\n",
        "    <tr>\n",
        "    <td>\n",
        "    **2 Layer Train Accuracy**\n",
        "    </td>\n",
        "    <td>\n",
        "    0.9999999999999998\n",
        "    </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "    <td> **2 Layer Test Accuracy**</td>\n",
        "    <td> 0.74 </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "    <td> **4 Layer Train Accuracy**</td>\n",
        "    <td> 0.9856459330143539 </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "    <td> **4 Layer Test Accuracy**</td>\n",
        "    <td> 0.8 </td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsgiE9YDmrH1"
      },
      "source": [
        "Congratulations! Your 4-layer MLP performed better (80%) than your 2-layer MLP (72%) when evaluated with your test data. This is a good performance!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Part 8: Test with your own images**\n",
        "\n",
        "To test your model with your own images, you'll need to upload the images you want to test to your UFV Google Drive. Then configure the `image_path` variable in the cell below to point to the path of the image in your Google Drive. Note that the path to the root directory of your Google Drive is \"/content/gdrive/MyDrive/\". Therefore, if your image is in the root of your Google Drive and is called \"my_cat.jpg\", the variable should be configured as `image_path = /content/gdrive/MyDrive/my_cat.jpg`"
      ],
      "metadata": {
        "id": "P5lKU_WgCAvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "### YOUR CODE STARTS HERE ### ~1 line\n",
        "image_path = \"/content/gdrive/MyDrive/<path_to_img>.jpg\"\n",
        "### YOUR CODE ENDS HERE ###\n",
        "\n",
        "# We preprocess the image to fit your algorithm.\n",
        "with Image.open(image_path) as im:\n",
        "  low_res_image = np.array(im.resize((num_px, num_px)))\n",
        "\n",
        "x = low_res_image.reshape((num_px * num_px * 3, 1))\n",
        "y = predict(x, w, b)\n",
        "\n",
        "plt.imshow(low_res_image)\n",
        "print(\"y = \" + str(np.squeeze(y)) + \", your algorithm predicts a \\\"\" + classes[int(np.squeeze(y)),].decode(\"utf-8\") +  \"\\\" picture.\")"
      ],
      "metadata": {
        "id": "Hc_vxmsECIG7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}